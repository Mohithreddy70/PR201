{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb07x7Qv-HXO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7LhX-N3-Wvr",
        "outputId": "49e5f8f4-34b2-4212-8f71-5d161d27f43d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\http\\client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
            "    status = _inner_run()\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
            "    return self.run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/einops/\n",
            "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl.metadata\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/torch/\n",
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\http\\client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
            "    status = _inner_run()\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
            "    return self.run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"C:\\Users\\ksair\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q einops\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q torchvision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# ðŸ“Œ Setup\n",
        "!pip install -q timm\n",
        "!pip install -q einops\n",
        "!pip install -q torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from einops import rearrange\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5BTrWOC-Zb0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/brain_tumour.zip\"  # Update this path as needed\n",
        "extract_path = \"/content/dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmuaWcGE-bbH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class LungTumorDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1ZbRK77-dR1"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = LungTumorDataset(\n",
        "    image_dir=\"/content/dataset/images\",\n",
        "    mask_dir=\"/content/dataset/masks\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.8 * len(dataset))\n",
        "val_len = len(dataset) - train_len\n",
        "train_set, val_set = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suobAmbG-gcq"
      },
      "outputs": [],
      "source": [
        "# âœ… Deformable Convolution Placeholder (replace with actual if using mmcv or deformable conv support)\n",
        "class DeformableConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # NOTE: Actual deformable conv is not implemented here.\n",
        "        return self.conv(x)\n",
        "\n",
        "# âœ… Weight Generation Unit\n",
        "class WeightGenUnit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.fc(x)\n",
        "        return x * w\n",
        "\n",
        "# âœ… Double Conv Block\n",
        "def double_conv(in_ch, out_ch):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "# âœ… WDU-Net Block with WGU + Deformable Conv\n",
        "class WDUBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "        self.wgu = WeightGenUnit(out_ch, out_ch)\n",
        "        self.deform = DeformableConv2D(out_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.wgu(x)\n",
        "        x = self.deform(x)\n",
        "        return x\n",
        "\n",
        "# âœ… WDU-Net Architecture\n",
        "class WDUNet(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=1):\n",
        "        super().__init__()\n",
        "        self.enc1 = WDUBlock(in_ch, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = WDUBlock(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = WDUBlock(128, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = WDUBlock(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = WDUBlock(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        b = self.bottleneck(self.pool2(e2))\n",
        "        d2 = self.dec2(torch.cat([self.up2(b), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "        return self.final(d1)\n",
        "\n",
        "# âœ… Focal Asymmetric Similarity Loss (FASL)\n",
        "class FASLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.8, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
        "        FAS = self.alpha * (1 - pt) ** self.gamma * BCE\n",
        "        return FAS.mean()\n",
        "\n",
        "# âœ… Dummy Training Loop\n",
        "def train_step(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "        pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# âœ… Dummy Inference with Ensemble (if multiple models are trained)\n",
        "def ensemble_inference(models, x):\n",
        "    with torch.no_grad():\n",
        "        preds = [torch.sigmoid(m(x.cuda())) for m in models]\n",
        "        return torch.mean(torch.stack(preds), dim=0)\n",
        "\n",
        "# Example Usage\n",
        "# model = WDUNet().cuda()\n",
        "# criterion = FASLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ymv1j1B-kKR"
      },
      "outputs": [],
      "source": [
        "model = WDUNet().cuda()\n",
        "criterion = FASLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OElmSYk1-mOG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            imgs, masks = imgs.cuda(), masks.cuda()\n",
        "            preds = model(imgs)\n",
        "            loss = criterion(preds, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        evaluate_model(model, val_loader)\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    iou_total = 0\n",
        "    f_measure_total = 0\n",
        "    mae_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.cuda(), masks.cuda()\n",
        "            preds = model(imgs)\n",
        "            preds_sigmoid = torch.sigmoid(preds)\n",
        "            preds_binary = (preds_sigmoid > 0.5).float()\n",
        "\n",
        "            intersection = (preds_binary * masks).sum((1, 2, 3))\n",
        "            union = ((preds_binary + masks) > 0).float().sum((1, 2, 3))\n",
        "            iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "            iou_total += iou.mean().item()\n",
        "\n",
        "            # Calculate F-measure and MAE for the batch\n",
        "            f_beta_batch = f_measure(preds_binary, masks)\n",
        "            f_measure_total += f_beta_batch.mean().item()\n",
        "\n",
        "            mae_batch = mae(preds_sigmoid, masks)\n",
        "            mae_total += mae_batch.mean().item()\n",
        "\n",
        "\n",
        "    avg_iou = iou_total / len(val_loader)\n",
        "    avg_f_measure = f_measure_total / len(val_loader)\n",
        "    avg_mae = mae_total / len(val_loader)\n",
        "\n",
        "    print(f\"Validation IoU: {avg_iou:.4f}, F-measure: {avg_f_measure:.4f}, MAE: {avg_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8NjAES5-q7t",
        "outputId": "c07832af-2626-4af6-ab00-71876701b7a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:23<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0134\n",
            "Validation IoU: 0.1058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:26<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0089\n",
            "Validation IoU: 0.2863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0077\n",
            "Validation IoU: 0.3178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0067\n",
            "Validation IoU: 0.3985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0063\n",
            "Validation IoU: 0.4440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0058\n",
            "Validation IoU: 0.4775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:28<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0053\n",
            "Validation IoU: 0.4316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0052\n",
            "Validation IoU: 0.4586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:26<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0049\n",
            "Validation IoU: 0.5029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0046\n",
            "Validation IoU: 0.4976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0044\n",
            "Validation IoU: 0.4069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0041\n",
            "Validation IoU: 0.5191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0040\n",
            "Validation IoU: 0.5319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0037\n",
            "Validation IoU: 0.4926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 613/613 [02:25<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0036\n",
            "Validation IoU: 0.5643\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GMhljcxNwKu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBVx3KvKKFLs"
      },
      "outputs": [],
      "source": [
        "def f_measure(pred, gt, beta_squared=0.3):\n",
        "    \"\"\"\n",
        "    Computes the FÎ²-score between predicted and ground truth masks for a batch.\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor): Binary prediction mask (0 or 1), shape [N, C, H, W]\n",
        "        gt (torch.Tensor): Binary ground truth mask (0 or 1), shape [N, C, H, W]\n",
        "        beta_squared (float): Beta^2 parameter. Defaults to 0.3.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: FÎ²-scores for each item in the batch\n",
        "    \"\"\"\n",
        "    pred = pred.float()\n",
        "    gt = gt.float()\n",
        "\n",
        "    # Ensure shapes match (assuming single channel for now)\n",
        "    if pred.shape[-2:] != gt.shape[-2:]:\n",
        "        pred = F.interpolate(pred, size=gt.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "    TP = (pred * gt).sum(dim=(-2, -1))\n",
        "    precision = TP / (pred.sum(dim=(-2, -1)) + 1e-8)\n",
        "    recall = TP / (gt.sum(dim=(-2, -1)) + 1e-8)\n",
        "\n",
        "    f_beta = ((1 + beta_squared) * precision * recall) / (beta_squared * precision + recall + 1e-8)\n",
        "    return f_beta\n",
        "\n",
        "def mae(pred, gt):\n",
        "    \"\"\"\n",
        "    Computes Mean Absolute Error between predicted and ground truth masks for a batch.\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor): Predicted mask, values in [0, 1], shape [N, C, H, W]\n",
        "        gt (torch.Tensor): Ground truth mask, values in [0, 1], shape [N, C, H, W]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: MAE values for each item in the batch\n",
        "    \"\"\"\n",
        "    pred = pred.float()\n",
        "    gt = gt.float()\n",
        "\n",
        "    # Ensure shapes match (assuming single channel for now)\n",
        "    if pred.shape[-2:] != gt.shape[-2:]:\n",
        "        pred = F.interpolate(pred, size=gt.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "    return torch.abs(pred - gt).mean(dim=(-2, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Cnrg4tKMjp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5PP7u-K-o8X",
        "outputId": "0324b1b5-1bda-4296-fafb-3e3ce4af9685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WDUNet(\n",
              "  (enc1): WDUBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (wgu): WeightGenUnit(\n",
              "      (fc): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (deform): DeformableConv2D(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (enc2): WDUBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (wgu): WeightGenUnit(\n",
              "      (fc): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (deform): DeformableConv2D(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): WDUBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (wgu): WeightGenUnit(\n",
              "      (fc): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (deform): DeformableConv2D(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec2): WDUBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (wgu): WeightGenUnit(\n",
              "      (fc): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (deform): DeformableConv2D(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec1): WDUBlock(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (wgu): WeightGenUnit(\n",
              "      (fc): Sequential(\n",
              "        (0): AdaptiveAvgPool2d(output_size=1)\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (deform): DeformableConv2D(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), \"wdu_net.pth\")\n",
        "\n",
        "# Load\n",
        "model.load_state_dict(torch.load(\"wdu_net.pth\"))\n",
        "model.eval()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
